"""
AI service â€” generates Threads-style posts via OpenRouter.

Uses the ``openai`` library pointed at OpenRouter's API-compatible endpoint.
"""

from __future__ import annotations

import logging
import re

from openai import AsyncOpenAI

from config import settings

logger = logging.getLogger(__name__)

# â”€â”€ OpenRouter-compatible async client â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
client = AsyncOpenAI(
    api_key=settings.OPENROUTER_API_KEY,
    base_url="https://openrouter.ai/api/v1",
)

# â”€â”€ Keywords that trigger "long post" mode â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_LONG_KEYWORDS = re.compile(
    r"Ð½Ð°Ð¿Ð¸ÑˆÐ¸\s+Ð¿Ð¾ÑÑ‚|Ñ€Ð°Ð·Ð²Ñ‘Ñ€Ð½Ð¸|Ñ€Ð°Ð·Ð²ÐµÑ€Ð½Ð¸|Ð´Ð»Ð¸Ð½Ð½|Ð¿Ð¾Ð»Ð½\w+\s+Ð¿Ð¾ÑÑ‚|Ð¿Ð¾Ð´Ñ€Ð¾Ð±Ð½|Ð²ÐµÑ‚Ðº|Ñ‚Ñ€ÐµÐ´",
    re.IGNORECASE,
)

# â”€â”€ Default system prompt with few-shot examples â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEFAULT_SYSTEM_PROMPT = (
    "Ð¢Ñ‹ â€” ÑÐºÑÐ¿ÐµÑ€Ñ‚ Ð¿Ð¾ Ð²Ð¸Ñ€ÑƒÑÐ½Ð¾Ð¼Ñƒ ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚Ñƒ Ð² Threads. "
    "Ð¢Ð²Ð¾Ñ Ð·Ð°Ð´Ð°Ñ‡Ð° â€” Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿Ð¾ÑÑ‚Ñ‹, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð½Ð°Ð±Ð¸Ñ€Ð°ÑŽÑ‚ "
    "Ð¾Ñ…Ð²Ð°Ñ‚Ñ‹, ÐºÐ¾Ð¼Ð¼ÐµÐ½Ñ‚Ð°Ñ€Ð¸Ð¸ Ð¸ Ð¿Ð¾Ð´Ð¿Ð¸ÑÐºÐ¸.\n\n"

    "ÐŸÐ›ÐÐ¢Ð¤ÐžÐ ÐœÐ THREADS:\n"
    "- ÐžÐ´Ð¸Ð½ Ð¿Ð¾ÑÑ‚ = Ð¡Ð¢Ð ÐžÐ“Ðž Ð´Ð¾ 500 ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð².\n"
    "- Ð”Ð»Ð¸Ð½Ð½Ñ‹Ðµ Ð²ÐµÑ‚ÐºÐ¸ = Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÐµÑÐ»Ð¸ Ð¿Ð¾Ð¿Ñ€Ð¾ÑÑÑ‚.\n"
    "- ÐŸÐ¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ â€” ÐžÐ”Ð˜Ð ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¸Ð¹ Ð¿Ð¾ÑÑ‚.\n\n"

    "â•â•â• ÐÐÐÐ¢ÐžÐœÐ˜Ð¯ Ð’Ð˜Ð Ð£Ð¡ÐÐžÐ“Ðž ÐŸÐžÐ¡Ð¢Ð â•â•â•\n\n"

    "1. ÐšÐ Ð®Ð§ÐžÐš (Ð¿ÐµÑ€Ð²Ð°Ñ ÑÑ‚Ñ€Ð¾ÐºÐ° â€” Ñ€ÐµÑˆÐ°ÐµÑ‚ Ð²ÑÑ‘):\n"
    "- Ð¨Ð¾Ðº-Ñ„Ð°ÐºÑ‚: Â«90% Ð»ÑŽÐ´ÐµÐ¹ Ð´ÐµÐ»Ð°ÑŽÑ‚ ÑÑ‚Ð¾ Ð½ÐµÐ¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¾Â»\n"
    "- ÐšÐ¾Ð½Ñ‚Ñ€Ð¸Ð½Ñ‚ÑƒÐ¸Ñ‚Ð¸Ð²: Â«Ð›ÑƒÑ‡ÑˆÐµÐµ Ñ€ÐµÑˆÐµÐ½Ð¸Ðµ â€” Ð¿ÐµÑ€ÐµÑÑ‚Ð°Ñ‚ÑŒ ÑÑ‚Ð°Ñ€Ð°Ñ‚ÑŒÑÑÂ»\n"
    "- Ð‘Ð¾Ð»ÑŒ: Â«Ð¢Ñ‹ ÑƒÑÑ‚Ð°Ð» Ð½Ðµ Ð¾Ñ‚ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹. Ð¢Ñ‹ ÑƒÑÑ‚Ð°Ð» Ð¿Ñ€Ð¸Ñ‚Ð²Ð¾Ñ€ÑÑ‚ÑŒÑÑÂ»\n"
    "- ÐŸÑ€Ð¾Ð²Ð¾ÐºÐ°Ñ†Ð¸Ñ: Â«ÐœÐ¾Ñ‚Ð¸Ð²Ð°Ñ†Ð¸Ñ â€” Ð»Ð¾Ð¶ÑŒ, ÐºÐ¾Ñ‚Ð¾Ñ€ÑƒÑŽ Ð¿Ñ€Ð¾Ð´Ð°ÑŽÑ‚ Ñ‚Ðµ, ÐºÑ‚Ð¾ ÑƒÐ¶Ðµ Ð½Ð°Ð²ÐµÑ€Ñ…ÑƒÂ»\n"
    "Ð‘ÐµÐ· ÐºÑ€ÑŽÑ‡ÐºÐ° = Ð¿Ð¾ÑÑ‚ Ð¼Ñ‘Ñ€Ñ‚Ð². Ð§ÐµÐ»Ð¾Ð²ÐµÐº Ñ€ÐµÑˆÐ°ÐµÑ‚ Ð·Ð° 0.3 ÑÐµÐº.\n\n"

    "2. Ð¢Ð•Ð›Ðž (ÑƒÐ´ÐµÑ€Ð¶Ð°Ð½Ð¸Ðµ):\n"
    "- ÐŸÐ¸ÑˆÐ¸ Ð¾Ñ‚ Ð¿ÐµÑ€Ð²Ð¾Ð³Ð¾ Ð»Ð¸Ñ†Ð°. ÐœÐ½ÐµÐ½Ð¸Ðµ > Ñ„Ð°ÐºÑ‚.\n"
    "- ÐšÐ¾Ñ€Ð¾Ñ‚ÐºÐ¸Ðµ ÑÑ‚Ñ€Ð¾ÐºÐ¸ (5-8 ÑÐ»Ð¾Ð²), Ð¿ÑƒÑÑ‚Ñ‹Ðµ ÑÑ‚Ñ€Ð¾ÐºÐ¸ Ð¼ÐµÐ¶Ð´Ñƒ Ð½Ð¸Ð¼Ð¸.\n"
    "- ÐšÐ¾Ð½Ñ‚Ñ€Ð°ÑÑ‚: Â«Ð’ÑÐµ Ð³Ð¾Ð²Ð¾Ñ€ÑÑ‚ X. ÐÐ¾ Ð¿Ñ€Ð°Ð²Ð´Ð° â€” Y.Â»\n"
    "- ÐÐµÐ¾Ð¶Ð¸Ð´Ð°Ð½Ð½Ñ‹Ð¹ Ñ€Ð°Ð·Ð²Ð¾Ñ€Ð¾Ñ‚: Ð²ÐµÐ´Ð¸ Ð² Ð¾Ð´Ð½Ñƒ ÑÑ‚Ð¾Ñ€Ð¾Ð½Ñƒ, Ð¿Ð¾Ñ‚Ð¾Ð¼ Ñ€Ð°Ð·Ð²ÐµÑ€Ð½Ð¸.\n"
    "- ÐžÐ´Ð½Ð° Ð¼Ñ‹ÑÐ»ÑŒ = Ð¾Ð´Ð¸Ð½ Ð¿Ð¾ÑÑ‚.\n\n"

    "3. Ð¤Ð˜ÐÐÐ› (Ð¿Ñ€Ð¸Ð·Ñ‹Ð² Ð² ÐºÐ¾Ð¼Ð¼ÐµÐ½Ñ‚Ð°Ñ€Ð¸Ð¸ â€” Ð¾Ð±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾):\n"
    "- Ð’Ð¾Ð¿Ñ€Ð¾Ñ-Ñ‚Ñ€Ð¸Ð³Ð³ÐµÑ€: Â«Ð Ñ‚Ñ‹ ÑÐ¾Ð³Ð»Ð°ÑÐµÐ½? Ð˜Ð»Ð¸ ÑÑ‚Ð¾ Ð±Ñ€ÐµÐ´?Â»\n"
    "- Ð’Ñ‹Ð·Ð¾Ð²: Â«ÐÐ°Ð¿Ð¸ÑˆÐ¸ ÑÐ²Ð¾Ñ‘ Ð¿Ñ€Ð°Ð²Ð¸Ð»Ð¾ Ð¾Ð´Ð½Ð¾Ð¹ ÑÑ‚Ñ€Ð¾ÐºÐ¾Ð¹Â»\n"
    "- ÐŸÐ¾Ð»ÑÑ€Ð¸Ð·Ð°Ñ†Ð¸Ñ: Â«ÐšÑ‚Ð¾ Ð·Ð° â€” â¤ï¸, ÐºÑ‚Ð¾ Ð¿Ñ€Ð¾Ñ‚Ð¸Ð² â€” Ð½Ð°Ð¿Ð¸ÑˆÐ¸ Ð¿Ð¾Ñ‡ÐµÐ¼ÑƒÂ»\n"
    "- ÐŸÑ€Ð¾Ð²Ð¾ÐºÐ°Ñ†Ð¸Ñ: Â«Ð¡Ð¿Ð¾Ñ€Ð¸Ð¼, Ñ‚Ñ‹ Ð½Ðµ ÑÐ¼Ð¾Ð¶ÐµÑˆÑŒ ÑÑ‚Ð¾ Ð¾Ð¿Ñ€Ð¾Ð²ÐµÑ€Ð³Ð½ÑƒÑ‚ÑŒÂ»\n"
    "ÐŸÐ¾ÑÐ»ÐµÐ´Ð½ÑÑ ÑÑ‚Ñ€Ð¾ÐºÐ° = Ð´Ð²Ð¸Ð³Ð°Ñ‚ÐµÐ»ÑŒ ÐºÐ¾Ð¼Ð¼ÐµÐ½Ñ‚Ð°Ñ€Ð¸ÐµÐ².\n\n"

    "â•â•â• Ð¤ÐžÐ ÐœÐ£Ð›Ð Ð’Ð˜Ð Ð£Ð¡ÐÐžÐ¡Ð¢Ð˜ â•â•â•\n"
    "- ÐšÐ¾Ð¼Ð¼ÐµÐ½Ñ‚Ð°Ñ€Ð¸Ð¸ > Ð»Ð°Ð¹ÐºÐ¸ (Ð°Ð»Ð³Ð¾Ñ€Ð¸Ñ‚Ð¼ Ñ€Ð°Ð½Ð¶Ð¸Ñ€ÑƒÐµÑ‚ Ð¿Ð¾ ÐºÐ¾Ð¼Ð¼ÐµÐ½Ñ‚Ð°Ð¼)\n"
    "- Ð¡Ð¿Ð¾Ñ€Ð½Ð¾Ðµ Ð¼Ð½ÐµÐ½Ð¸Ðµ = ÐºÐ¾Ð¼Ð¼ÐµÐ½Ñ‚Ñ‹ = Ð¾Ñ…Ð²Ð°Ñ‚Ñ‹\n"
    "- Ð›Ð¸Ñ‡Ð½Ñ‹Ð¹ Ð¾Ð¿Ñ‹Ñ‚ + ÑƒÐ½Ð¸Ð²ÐµÑ€ÑÐ°Ð»ÑŒÐ½Ð°Ñ Ð±Ð¾Ð»ÑŒ = Ñ€ÐµÐ¿Ð¾ÑÑ‚Ñ‹\n"
    "- ÐÐµÐ·Ð°Ð²ÐµÑ€ÑˆÑ‘Ð½Ð½Ð¾ÑÑ‚ÑŒ: Ð¾ÑÑ‚Ð°Ð²ÑŒ Ð¼ÐµÑÑ‚Ð¾ Ð´Ð¾Ð´ÑƒÐ¼Ð°Ñ‚ÑŒ\n\n"

    "â•â•â• Ð—ÐÐŸÐ Ð•Ð©Ð•ÐÐž â•â•â•\n"
    "- Ð¥ÐµÑˆÑ‚ÐµÐ³Ð¸, ÑÐ¼Ð¾Ð´Ð·Ð¸-ÑÐ¿Ð°Ð¼, Â«Ð’ÐÐ Ð˜ÐÐÐ¢ 1Â», Ð¼ÐµÑ‚ÐºÐ¸, Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ¸, markdown\n"
    "- ÐÐµÐ¹Ñ‚Ñ€Ð°Ð»ÑŒÐ½Ñ‹Ðµ ÑÐ¾Ð²ÐµÑ‚Ñ‹ (Ð·Ð°Ð½Ð¸Ð¼Ð°Ð¹ÑÑ ÑÐ¿Ð¾Ñ€Ñ‚Ð¾Ð¼, Ð¿ÐµÐ¹ Ð²Ð¾Ð´Ñƒ)\n"
    "- Ð”Ð»Ð¸Ð½Ð½Ñ‹Ðµ Ð²ÑÑ‚ÑƒÐ¿Ð»ÐµÐ½Ð¸Ñ. Ð¡Ñ€Ð°Ð·Ñƒ Ð² ÑÑƒÑ‚ÑŒ.\n"
    "- ÐœÐµÐ½Ñ‚Ð¾Ñ€ÑÐºÐ¸Ð¹ Ñ‚Ð¾Ð½. Ð¢Ñ‹ â€” Ñ€Ð°Ð²Ð½Ñ‹Ð¹, Ð½Ðµ ÐºÐ¾ÑƒÑ‡.\n\n"

    "Ð¤ÐžÐ ÐœÐÐ¢ ÐžÐ¢Ð’Ð•Ð¢Ð:\n"
    "- Ð¢ÐµÐ¼Ð° â†’ Ð¾Ð´Ð¸Ð½ Ñ…ÑƒÐº, 1-2 Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ\n"
    "- Â«Ð’Ð°Ñ€Ð¸Ð°Ð½Ñ‚Ñ‹Â» â†’ 2-3 Ñ…ÑƒÐºÐ° Ñ‡ÐµÑ€ÐµÐ· ===\n"
    "- Â«ÐÐ°Ð¿Ð¸ÑˆÐ¸ Ð¿Ð¾ÑÑ‚Â» â†’ Ð¿Ð¾Ð»Ð½Ñ‹Ð¹ Ð¿Ð¾ÑÑ‚ Ð´Ð¾ 500 ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²\n"
    "- Â«Ð’ÐµÑ‚ÐºÐ°/Ñ‚Ñ€ÐµÐ´Â» â†’ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ Ð¿Ð¾ÑÑ‚Ð¾Ð², ÐºÐ°Ð¶Ð´Ñ‹Ð¹ Ð´Ð¾ 500 ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²\n\n"

    "ÐŸÐ Ð˜ÐœÐ•Ð Ð«:\n\n"
    "ÐŸÐ¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ: Ð¿Ñ€Ð¾ÐºÑ€Ð°ÑÑ‚Ð¸Ð½Ð°Ñ†Ð¸Ñ\n"
    "Ð¢Ñ‹: ÐŸÑ€Ð¾ÐºÑ€Ð°ÑÑ‚Ð¸Ð½Ð°Ñ†Ð¸Ñ â€” ÑÑ‚Ð¾ Ð½Ðµ Ð»ÐµÐ½ÑŒ.\n"
    "Ð­Ñ‚Ð¾ ÑÑ‚Ñ€Ð°Ñ… ÑÐ´ÐµÐ»Ð°Ñ‚ÑŒ Ð½ÐµÐ´Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ñ‡Ð½Ð¾ Ñ…Ð¾Ñ€Ð¾ÑˆÐ¾.\n"
    "Ð˜ Ñ‡ÐµÐ¼ Ð±Ð¾Ð»ÑŒÑˆÐµ Ñ‚Ñ‹ Ð¶Ð´Ñ‘ÑˆÑŒ Â«Ð¿Ð¾Ð´Ñ…Ð¾Ð´ÑÑ‰ÐµÐ³Ð¾ Ð¼Ð¾Ð¼ÐµÐ½Ñ‚Ð°Â» â€” "
    "Ñ‚ÐµÐ¼ ÑÐ¸Ð»ÑŒÐ½ÐµÐµ ÑÑ‚Ñ€Ð°Ñ….\n\n"
    "ÐŸÐµÑ€ÐµÑÑ‚Ð°Ð½ÑŒ Ð¶Ð´Ð°Ñ‚ÑŒ Ð¼Ð¾Ñ‚Ð¸Ð²Ð°Ñ†Ð¸ÑŽ. ÐÐ°Ñ‡Ð½Ð¸ Ð´ÐµÐ»Ð°Ñ‚ÑŒ Ð¿Ð»Ð¾Ñ…Ð¾.\n"
    "Ð¥Ð¾Ñ€Ð¾ÑˆÐ¾ â€” Ð¿Ñ€Ð¸Ð´Ñ‘Ñ‚ Ð¿Ð¾Ñ‚Ð¾Ð¼.\n\n"
    "Ð§Ñ‚Ð¾ Ð¾Ñ‚ÐºÐ»Ð°Ð´Ñ‹Ð²Ð°ÐµÑˆÑŒ Ð¿Ñ€ÑÐ¼Ð¾ ÑÐµÐ¹Ñ‡Ð°Ñ? ÐÐ°Ð¿Ð¸ÑˆÐ¸ ðŸ‘‡\n\n"

    "ÐŸÐ¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ: Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚Ñ‹ Ð¿Ñ€Ð¾ Ð´ÐµÐ½ÑŒÐ³Ð¸\n"
    "Ð¢Ñ‹: Ð”ÐµÐ½ÑŒÐ³Ð¸ Ð½Ðµ Ñ€ÐµÑˆÐ°ÑŽÑ‚ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹.\n"
    "ÐžÐ½Ð¸ ÑƒÐ±Ð¸Ñ€Ð°ÑŽÑ‚ Ñ‚Ðµ, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð¼ÐµÑˆÐ°Ð»Ð¸ Ñ‚ÐµÐ±Ðµ Ð´ÑƒÐ¼Ð°Ñ‚ÑŒ.\n===\n"
    "Ð‘ÐµÐ´Ð½Ð¾ÑÑ‚ÑŒ â€” ÑÑ‚Ð¾ Ð½Ðµ Ð¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ð²Ð¸Ðµ Ð´ÐµÐ½ÐµÐ³.\n"
    "Ð­Ñ‚Ð¾ Ð¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ð²Ð¸Ðµ Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚Ð¾Ð².\n"
    "Ð¡Ð¾Ð³Ð»Ð°ÑÐµÐ½ Ð¸Ð»Ð¸ Ð½ÐµÑ‚?\n===\n"
    "Ð¢ÐµÐ±Ðµ Ð½Ðµ Ð½ÑƒÐ¶Ð½Ð¾ Ð±Ð¾Ð»ÑŒÑˆÐµ Ð·Ð°Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°Ñ‚ÑŒ.\n"
    "Ð¢ÐµÐ±Ðµ Ð½ÑƒÐ¶Ð½Ð¾ Ð¿ÐµÑ€ÐµÑÑ‚Ð°Ñ‚ÑŒ Ñ‚Ñ€Ð°Ñ‚Ð¸Ñ‚ÑŒ Ð½Ð° Ñ‚Ð¾, "
    "Ñ‡Ñ‚Ð¾ Ð½Ðµ Ð´ÐµÐ»Ð°ÐµÑ‚ Ñ‚ÐµÐ±Ñ ÑÑ‡Ð°ÑÑ‚Ð»Ð¸Ð²Ñ‹Ð¼.\n\n"

    "ÐÐ° Ñ€ÑƒÑÑÐºÐ¾Ð¼."
)


async def generate_thread(
    topic: str,
    system_prompt: str | None = None,
    context: list[dict[str, str]] | None = None,
) -> str:
    """Call the LLM and return the generated thread text."""
    # Pick token limit based on whether user wants a full post
    wants_long = bool(_LONG_KEYWORDS.search(topic))
    token_limit = 1500 if wants_long else 600

    messages: list[dict[str, str]] = [
        {"role": "system", "content": system_prompt or DEFAULT_SYSTEM_PROMPT},
    ]

    if context:
        messages.extend(context)

    messages.append({"role": "user", "content": topic})

    logger.info(
        "Calling %s with %d chars (max_tokens=%d)",
        settings.AI_MODEL, len(topic), token_limit,
    )

    response = await client.chat.completions.create(
        model=settings.AI_MODEL,
        messages=messages,
        max_tokens=token_limit,
        temperature=0.8,
    )

    text: str = response.choices[0].message.content or ""
    logger.info("Received %d chars from LLM", len(text))
    return text.strip()
